{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_part_clean4.csv\")\n",
    "df_full = df[[\"INCIDENT_DATETIME\", \"ZIPCODE\", \"NEIGHBORHOOD\"]]\n",
    "df_full[\"BOROUGH\"] = \"Manhattan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "neighborhood_or_zipcode = [\"BOROUGH\", \"NEIGHBORHOOD\", \"ZIPCODE\"]\n",
    "delta_aggregs = [1,4,12] # number of hours for aggregations\n",
    "delta_weeks = [1,2,3,4] # number of weeks for lags\n",
    "delta_years = [0,1,2] # number of years for weekly lags\n",
    "forecasting_horizon = 1 # forecasting horizon\n",
    "roll_split = 4 # divider for train and test set\n",
    "hours_year= 8736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_delta_list=[1] # list of all the shifts\n",
    "for delta_aggreg in delta_aggregs:\n",
    "    for delta_year in delta_years:\n",
    "        for delta_week in delta_weeks:\n",
    "            shift_delta_list.append(int((delta_year*hours_year+delta_week*7*24)/delta_aggreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables for shifts\n",
    "count_lag_columns = []\n",
    "neig_lag_columns = []\n",
    "bor_lag_columns = []\n",
    "zip_lag_columns = []\n",
    "for element in shift_delta_list:\n",
    "    count_lag_columns.append(\"COUNT_LAG_\"+str(element))\n",
    "    neig_lag_columns.append(\"NEIG_LAG_\"+str(element))\n",
    "    bor_lag_columns.append(\"BOR_LAG_\"+str(element))\n",
    "    zip_lag_columns.append(\"ZIP_LAG_\"+str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating population density reference for additional variables\n",
    "# hourly population density for manhattan districts is taken from Manhattan Population Explorer (https://github.com/citrusvanilla/manhattanpopulationexplorer)\n",
    "density_reference = pd.read_csv(\"density_reference3.csv\", delimiter=\";\") # load reference table\n",
    "columns_list = density_reference.columns.tolist()[1:] # create list of column names\n",
    "density_reference = pd.melt(density_reference, id_vars=\"nid\", value_vars=columns_list, var_name=\"HOUR\", value_name=\"POPULATION\") # wide to long format\n",
    "density_reference.HOUR = pd.to_numeric(density_reference.HOUR) # reformat datatype\n",
    "density_reference.POPULATION = pd.to_numeric(density_reference.POPULATION) # reformat datatype\n",
    "density_reference[\"WEEKDAY\"] = np.floor(density_reference.HOUR/24) # new column for weekday number\n",
    "density_reference[\"HOUR\"] = density_reference.HOUR%24 # make hours count to 23 instead of 167\n",
    "density_reference[\"1H\"] = (density_reference.HOUR//1)*1 # add aggregation sum\n",
    "density_reference[\"4H\"] = (density_reference.HOUR//4)*4 # add aggregation sum\n",
    "density_reference[\"12H\"] = (density_reference.HOUR//12)*12 # add aggregation sum\n",
    "density_reference_1H = density_reference.groupby([\"nid\", \"WEEKDAY\", \"1H\"]).POPULATION.sum().reset_index() # create reference table\n",
    "density_reference_4H = density_reference.groupby([\"nid\", \"WEEKDAY\", \"4H\"]).POPULATION.sum().reset_index() # create reference table\n",
    "density_reference_12H = density_reference.groupby([\"nid\", \"WEEKDAY\", \"12H\"]).POPULATION.sum().reset_index() # create reference table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tables for NEIGHBORHOOD and ZIPCODE and saving them in df_list\n",
    "df_list = {}\n",
    "for neig_zip in neighborhood_or_zipcode:\n",
    "    for aggregation_size in delta_aggregs:\n",
    "        print(\"working on creating\",neig_zip, str(aggregation_size)+\"H\")\n",
    "        df = df_full[[\"INCIDENT_DATETIME\", neig_zip]]\n",
    "        df[\"COUNT\"] = pd.Series([1 for x in range(len(df.index))])\n",
    "\n",
    "        df['INCIDENT_DATETIME'] = pd.to_datetime(df.INCIDENT_DATETIME)\n",
    "\n",
    "        df['INCIDENT_DATETIME'] = df.INCIDENT_DATETIME.dt.floor(str(aggregation_size)+\"H\")\n",
    "        df = df.groupby([neig_zip, 'INCIDENT_DATETIME']).COUNT.count().reset_index()\n",
    "        time = pd.date_range(df.INCIDENT_DATETIME.min(), df.INCIDENT_DATETIME.max(), freq=str(aggregation_size)+\"H\")\n",
    "\n",
    "        full = []\n",
    "        for neigh in df[neig_zip].unique():\n",
    "            full.append(pd.DataFrame({\n",
    "                'INCIDENT_DATETIME': time, neig_zip: [neigh]*len(time)\n",
    "        }))\n",
    "        full = pd.concat(full).reset_index(drop=True)\n",
    "\n",
    "        full = full.merge(df, on=[neig_zip, 'INCIDENT_DATETIME'], how='left')\n",
    "        full = full.fillna(0)\n",
    "        full.COUNT = full.COUNT.astype(\"int32\")\n",
    "        \n",
    "        name = \"df_\"+str(neig_zip)+\"_\"+str(aggregation_size)+\"H\"\n",
    "        df_list[name] = full\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sin and cos for cyclical feature encoding\n",
    "for element in list(df_list.keys()):\n",
    "    print(\"working on sin/cos for\",element)\n",
    "    df_list[element][\"HOUR\"] = df_list[element][\"INCIDENT_DATETIME\"].dt.hour\n",
    "    df_list[element][\"WEEKDAY\"] = df_list[element][\"INCIDENT_DATETIME\"].dt.weekday\n",
    "    df_list[element][\"MONTH\"] = df_list[element][\"INCIDENT_DATETIME\"].dt.month\n",
    "\n",
    "    df_list[element][\"HOUR_norm\"] = 2 * math.pi * df_list[element][\"HOUR\"] / df_list[element][\"HOUR\"].max()\n",
    "    df_list[element][\"HOUR_cos\"] = np.cos(df_list[element][\"HOUR_norm\"])\n",
    "    df_list[element][\"HOUR_sin\"] = np.sin(df_list[element][\"HOUR_norm\"])\n",
    "\n",
    "    df_list[element][\"WEEKDAY_norm\"] = 2 * math.pi * df_list[element][\"WEEKDAY\"] / df_list[element][\"WEEKDAY\"].max()\n",
    "    df_list[element][\"WEEKDAY_cos\"] = np.cos(df_list[element][\"WEEKDAY_norm\"])\n",
    "    df_list[element][\"WEEKDAY_sin\"] = np.sin(df_list[element][\"WEEKDAY_norm\"])\n",
    "\n",
    "    df_list[element][\"MONTH_norm\"] = 2 * math.pi * df_list[element][\"MONTH\"] / df_list[element][\"MONTH\"].max()\n",
    "    df_list[element][\"MONTH_cos\"] = np.cos(df_list[element][\"MONTH_norm\"])\n",
    "    df_list[element][\"MONTH_sin\"] = np.sin(df_list[element][\"MONTH_norm\"])\n",
    "\n",
    "# adding poopulation density\n",
    "    if \"NEIGHBORHOOD_1H\" in element:\n",
    "        df_list[element].NEIGHBORHOOD = df_list[element].NEIGHBORHOOD.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_1H, how=\"left\", left_on=[\"NEIGHBORHOOD\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"1H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"1H\"])\n",
    "    elif \"NEIGHBORHOOD_4H\" in element:\n",
    "        df_list[element].NEIGHBORHOOD = df_list[element].NEIGHBORHOOD.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_4H, how=\"left\", left_on=[\"NEIGHBORHOOD\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"4H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"4H\"])    \n",
    "    elif \"NEIGHBORHOOD_12H\" in element:\n",
    "        df_list[element].NEIGHBORHOOD = df_list[element].NEIGHBORHOOD.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_12H, how=\"left\", left_on=[\"NEIGHBORHOOD\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"12H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"12H\"])\n",
    "    elif \"ZIPCODE_1H\" in element:\n",
    "        df_list[element].ZIPCODE = df_list[element].ZIPCODE.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_1H, how=\"left\", left_on=[\"ZIPCODE\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"1H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"1H\"])\n",
    "    elif \"ZIPCODE_4H\" in element:\n",
    "        df_list[element].ZIPCODE = df_list[element].ZIPCODE.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_4H, how=\"left\", left_on=[\"ZIPCODE\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"4H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"4H\"])    \n",
    "    elif \"ZIPCODE_12H\" in element:\n",
    "        df_list[element].ZIPCODE = df_list[element].ZIPCODE.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_12H, how=\"left\", left_on=[\"ZIPCODE\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"12H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"12H\"])\n",
    "    elif \"BOROUGH_1H\" in element:\n",
    "        df_list[element].BOROUGH = df_list[element].BOROUGH.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_1H, how=\"left\", left_on=[\"BOROUGH\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"1H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"1H\"])\n",
    "    elif \"BOROUGH_4H\" in element:\n",
    "        df_list[element].BOROUGH = df_list[element].BOROUGH.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_4H, how=\"left\", left_on=[\"BOROUGH\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"4H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"4H\"])    \n",
    "    elif \"BOROUGH_12H\" in element:\n",
    "        df_list[element].BOROUGH = df_list[element].BOROUGH.astype(str)\n",
    "        df_list[element] = pd.merge(df_list[element], density_reference_12H, how=\"left\", left_on=[\"BOROUGH\", \"HOUR\", \"WEEKDAY\"], right_on = [\"nid\", \"12H\", \"WEEKDAY\"])\n",
    "        df_list[element] = df_list[element].drop(columns=[\"nid\", \"12H\"])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#adding lag\n",
    "for element in list(df_list.keys()):\n",
    "    if \"NEIGHBORHOOD\" in element:\n",
    "        print(\"working on lag for\",element)\n",
    "        # add lag\n",
    "        for count_lag, neig_lag, shift_delta in zip(count_lag_columns, neig_lag_columns, shift_delta_list):\n",
    "            df_list[element][neig_lag] = df_list[element][\"NEIGHBORHOOD\"].shift(shift_delta)\n",
    "            df_list[element][count_lag] = df_list[element][\"COUNT\"].shift(shift_delta)\n",
    "\n",
    "        # Lag Bleeding Correction\n",
    "        for count_lag, neig_lag in zip(count_lag_columns, neig_lag_columns):\n",
    "            df_list[element][count_lag] = np.where(df_list[element][\"NEIGHBORHOOD\"] != df_list[element][neig_lag], np.nan, df_list[element][count_lag])\n",
    "            df_list[element][neig_lag] = np.where(df_list[element][\"NEIGHBORHOOD\"] != df_list[element][neig_lag], np.nan, df_list[element][neig_lag])\n",
    "\n",
    "    elif \"ZIPCODE\" in element:\n",
    "        print(\"working on lag for\",element)\n",
    "        # add lag\n",
    "        for count_lag, zip_lag, shift_delta in zip(count_lag_columns, zip_lag_columns, shift_delta_list):\n",
    "            df_list[element][zip_lag] = df_list[element][\"ZIPCODE\"].shift(shift_delta)\n",
    "            df_list[element][count_lag] = df_list[element][\"COUNT\"].shift(shift_delta)\n",
    "\n",
    "        # Lag Bleeding Correction\n",
    "        for count_lag, zip_lag in zip(count_lag_columns, zip_lag_columns):\n",
    "            df_list[element][count_lag] = np.where(df_list[element][\"ZIPCODE\"] != df_list[element][zip_lag], np.nan, df_list[element][count_lag])\n",
    "            df_list[element][zip_lag] = np.where(df_list[element][\"ZIPCODE\"] != df_list[element][zip_lag], np.nan, df_list[element][zip_lag])\n",
    "\n",
    "    elif \"BOROUGH\" in element:\n",
    "        print(\"working on lag for\",element)\n",
    "        # add lag\n",
    "        for count_lag, bor_lag, shift_delta in zip(count_lag_columns, bor_lag_columns, shift_delta_list):\n",
    "            df_list[element][bor_lag] = df_list[element][\"BOROUGH\"].shift(shift_delta)\n",
    "            df_list[element][count_lag] = df_list[element][\"COUNT\"].shift(shift_delta)\n",
    "\n",
    "        # Lag Bleeding Correction\n",
    "        for count_lag, bor_lag in zip(count_lag_columns, bor_lag_columns):\n",
    "            df_list[element][count_lag] = np.where(df_list[element][\"BOROUGH\"] != df_list[element][bor_lag], np.nan, df_list[element][count_lag])\n",
    "            df_list[element][bor_lag] = np.where(df_list[element][\"BOROUGH\"] != df_list[element][bor_lag], np.nan, df_list[element][bor_lag])\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "# adding lag mean (MEDIC)\n",
    "lag_list_1=[] # list of all the shifts\n",
    "delta_aggregs = [1]\n",
    "for delta_aggreg in delta_aggregs:\n",
    "    for delta_year in delta_years:\n",
    "        for delta_week in delta_weeks:\n",
    "            lag_list_1.append(int((delta_year*hours_year+delta_week*7*24)/delta_aggreg))\n",
    "lag_columns_1 = []\n",
    "for item in lag_list_1:\n",
    "    lag_columns_1.append(\"COUNT_LAG_\"+str(item))\n",
    "\n",
    "lag_list_4=[] # list of all the shifts\n",
    "delta_aggregs = [4]\n",
    "for delta_aggreg in delta_aggregs:\n",
    "    for delta_year in delta_years:\n",
    "        for delta_week in delta_weeks:\n",
    "            lag_list_4.append(int((delta_year*hours_year+delta_week*7*24)/delta_aggreg))\n",
    "lag_columns_4 = []\n",
    "for item in lag_list_4:\n",
    "    lag_columns_4.append(\"COUNT_LAG_\"+str(item))\n",
    "\n",
    "lag_list_12=[] # list of all the shifts\n",
    "delta_aggregs = [12]\n",
    "for delta_aggreg in delta_aggregs:\n",
    "    for delta_year in delta_years:\n",
    "        for delta_week in delta_weeks:\n",
    "            lag_list_12.append(int((delta_year*hours_year+delta_week*7*24)/delta_aggreg))\n",
    "lag_columns_12 = []\n",
    "for item in lag_list_12:\n",
    "    lag_columns_12.append(\"COUNT_LAG_\"+str(item))\n",
    "\n",
    "\n",
    "for element in list(df_list.keys()):\n",
    "    print(\"working on lag mean for\",element)\n",
    "    df_list[element][\"MEAN_LAG_1\"] = df_list[element][lag_columns_1].mean(axis=1)\n",
    "    df_list[element][\"MEAN_LAG_4\"] = df_list[element][lag_columns_4].mean(axis=1)\n",
    "    df_list[element][\"MEAN_LAG_12\"] = df_list[element][lag_columns_12].mean(axis=1)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from darts import TimeSeries\n",
    "from darts.models import ExponentialSmoothing, NBEATSModel, AutoARIMA, Theta, ARIMA, NaiveSeasonal, Prophet, LightGBMModel, RandomForest, RNNModel, StatsForecastAutoARIMA, LinearRegressionModel\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results_counter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = list(df_list.keys())\n",
    "\n",
    "iterator_model = [\"NaiveSeasonal(K=24)\", \"NaiveSeasonal(K=6)\", \"NaiveSeasonal(K=2)\", \n",
    "                    \"ExponentialSmoothing()\", \n",
    "                    \"StatsForecastAutoARIMA()\", \n",
    "                    \"linear_model.LinearRegression()\",\n",
    "                    \"linear_model.LinearRegression()\",\n",
    "                    \"MEDIC\",\n",
    "                    \"MLPClassifier()\",\n",
    "                    \"MLPClassifier()\"]\n",
    "iterator_model_repeater = [\"none\", \"none\", \"none\", # naive\n",
    "                            \"none\", # exponential smoothing\n",
    "                            \"none\", # autoARIMA\n",
    "                            \"no_population\", # linearregression\n",
    "                            \"with_population\", # linearregression\n",
    "                            \"none\", #MEDIC\n",
    "                            \"no_population\", # MLPClassifier\n",
    "                            \"with_population\"] # MLPClassifier\n",
    "x_columns = [ \"HOUR_cos\", \"HOUR_sin\", \"WEEKDAY_cos\", \"WEEKDAY_sin\", \"MONTH_cos\", \"MONTH_sin\"]\n",
    "y_columns = [\"COUNT\"]\n",
    "date_column = [\"INCIDENT_DATETIME\"]\n",
    "medic_columns = [\"MEAN_LAG_1\", \"MEAN_LAG_4\", \"MEAN_LAG_12\"]\n",
    "population_columns = [\"POPULATION\"]\n",
    "\n",
    "for df_element in list_of_dfs: # selecting df from list of dfs\n",
    "    grouper = \"empty\" # variable for either NEIGHBORHOOD or ZIPCODE\n",
    "    if \"NEIGHBORHOOD\" in df_element:\n",
    "        grouper = \"NEIGHBORHOOD\"\n",
    "    elif \"ZIPCODE\" in df_element:\n",
    "        grouper = \"ZIPCODE\"\n",
    "    elif \"BOROUGH\" in df_element:\n",
    "        grouper = \"BOROUGH\"\n",
    "\n",
    "    list_of_groups = list(df_list[df_element][grouper].unique())\n",
    "\n",
    "    for group_element in list_of_groups:\n",
    "        series = TimeSeries.from_dataframe(df_list[df_element][df_list[df_element][grouper] == group_element], \"INCIDENT_DATETIME\", \"COUNT\")\n",
    "        series2 = df_list[df_element][df_list[df_element][grouper] == group_element][x_columns+y_columns+date_column]\n",
    "        series3 = df_list[df_element][df_list[df_element][grouper] == group_element][date_column+y_columns+medic_columns]\n",
    "        series4 = df_list[df_element][df_list[df_element][grouper] == group_element][x_columns+y_columns+date_column+population_columns]\n",
    "\n",
    "        series_total_len = len(series)\n",
    "        series_test_len = round(series_total_len/roll_split)\n",
    "        series_train_len = series_total_len-series_test_len\n",
    "\n",
    "        for model_name, model_repeater in zip(iterator_model, iterator_model_repeater): # selecting model from list of models\n",
    "            \n",
    "            # optimizer for calculation duration:\n",
    "            if \"1H\" in df_element:\n",
    "                series_test_len_temporary = range(4206, 4206+24+24+24)\n",
    "            elif \"4H\" in df_element:\n",
    "                series_test_len_temporary = range(1052, 1052+6+6+6)\n",
    "            elif \"12H\" in df_element:\n",
    "                series_test_len_temporary = range(350, 350+2+2+2)\n",
    "\n",
    "            for rolling_iterator in series_test_len_temporary: #############################################\n",
    "                # status output to console  \n",
    "                sys.stdout.write(\"\\r\")\n",
    "                sys.stdout.write(str(df_element)+ \" group_element: \"+str(group_element)+\"  iterator: \"+str(rolling_iterator+1)+\"  model: \"+str(model_name))  \n",
    "                \n",
    "                if \"NaiveSeasonal\" in model_name or \"ExponentialSmoothing\" in model_name or \"StatsForecastAutoARIMA\" in model_name:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    train, val = series[rolling_iterator:series_train_len+rolling_iterator], series[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon]\n",
    "                    model = eval(model_name)\n",
    "                    model.fit(train)\n",
    "                    prediction = model.predict(len(val))\n",
    "\n",
    "                    y_real = val.pd_dataframe().iat[0,0]\n",
    "                    y_predict = prediction.pd_dataframe().iat[0,0]\n",
    "                    y_Datetime = prediction.pd_dataframe().index[0].to_datetime64()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time-start_time\n",
    "                \n",
    "                elif \"LinearRegression\" in model_name and \"no_population\" in model_repeater:\n",
    "                    start_time = time.time()  \n",
    "\n",
    "                    train, val = series2[rolling_iterator:series_train_len+rolling_iterator], series2[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon]\n",
    "\n",
    "                    lm = eval(model_name)\n",
    "                    x = train[x_columns]\n",
    "                    y = train[y_columns]\n",
    "                    model = lm.fit(x,y)\n",
    "                    \n",
    "                    y_predict = lm.predict(val[x_columns]).tolist()[0][0] \n",
    "                    y_real = val[y_columns].iat[0,0]\n",
    "                    y_Datetime = val[date_column].iat[0,0] \n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    duration = end_time-start_time\n",
    "\n",
    "                elif \"LinearRegression\" in model_name and \"with_population\" in model_repeater:\n",
    "                    start_time = time.time()  \n",
    "\n",
    "                    train, val = series4[rolling_iterator:series_train_len+rolling_iterator], series4[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon]\n",
    "\n",
    "                    lm = eval(model_name)\n",
    "                    x = train[x_columns+population_columns]\n",
    "                    y = train[y_columns]\n",
    "                    model = lm.fit(x,y)\n",
    "                    \n",
    "                    y_predict = lm.predict(val[x_columns+population_columns]).tolist()[0][0] \n",
    "                    y_real = val[y_columns].iat[0,0]\n",
    "                    y_Datetime = val[date_column].iat[0,0] \n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    duration = end_time-start_time\n",
    "                \n",
    "                elif \"MEDIC\" in model_name:\n",
    "                    start_time = time.time()\n",
    "                    y_Datetime = \"1970-01-01 00:00:00\"\n",
    "                    y_real = 999\n",
    "                    y_predict = 999\n",
    "                    if \"1H\" in df_element:\n",
    "                        y_Datetime = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][date_column].iat[0,0]\n",
    "                        y_real = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][y_columns].iat[0,0]\n",
    "                        y_predict = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][medic_columns].iat[0,0]\n",
    "                    elif \"4H\" in df_element:\n",
    "                        y_Datetime = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][date_column].iat[0,0]\n",
    "                        y_real = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][y_columns].iat[0,0]\n",
    "                        y_predict = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][medic_columns].iat[0,1]\n",
    "                    elif \"12H\" in df_element:\n",
    "                        y_Datetime = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][date_column].iat[0,0]\n",
    "                        y_real = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][y_columns].iat[0,0]\n",
    "                        y_predict = series3[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon][medic_columns].iat[0,2]\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time-start_time\n",
    "                \n",
    "                elif \"MLPClassifier\" in model_name and \"no_population\" in model_repeater:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    train, val = series2[rolling_iterator:series_train_len+rolling_iterator], series2[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon]\n",
    "\n",
    "                    np.random.seed(2)\n",
    "                    model = MLPClassifier()\n",
    "                    model.fit(train[x_columns], np.ravel(train[y_columns]))\n",
    "                    \n",
    "                    y_predict = model.predict(val[x_columns])[0]\n",
    "                    y_real = val[y_columns].iat[0,0]\n",
    "                    y_Datetime = val[date_column].iat[0,0] \n",
    "\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time - start_time\n",
    "\n",
    "                elif \"MLPClassifier\" in model_name and \"with_population\" in model_repeater:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    train, val = series4[rolling_iterator:series_train_len+rolling_iterator], series4[series_train_len+rolling_iterator:series_train_len+rolling_iterator+forecasting_horizon]\n",
    "\n",
    "                    np.random.seed(2)\n",
    "                    model = MLPClassifier()\n",
    "                    model.fit(train[x_columns+population_columns], np.ravel(train[y_columns]))\n",
    "                    \n",
    "                    y_predict = model.predict(val[x_columns+population_columns])[0]\n",
    "                    y_real = val[y_columns].iat[0,0]\n",
    "                    y_Datetime = val[date_column].iat[0,0] \n",
    "\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time - start_time\n",
    "                \n",
    "                results.append({\"NH/ZIP\":df_element, \"Grouper\":grouper, \"Group_Element\":group_element, \"y_real\":y_real, \"y_predict\":y_predict, \"Datetime\":y_Datetime, \"Model\":model_name, \"Duration\":duration, \"Repeater\":model_repeater})\n",
    "            \n",
    "            print(str(\" done\"))\n",
    "\n",
    "        pd.DataFrame(results).to_csv(\"results_BOR_\"+str(results_counter)+\"_\"+str(df_element)+\"_\"+str(group_element)+\".csv\")\n",
    "        results = []\n",
    "        results_counter = results_counter +1\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisEng9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:29:02) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb6b1900154367daaed0083bf6fa5fd0989af355ea4688a27b496df8e301c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
